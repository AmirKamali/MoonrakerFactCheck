{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7pQDUhUnIo8"
      },
      "source": [
        "## MOONRAKER HACK 6124\n",
        "## Speach Fact Checker\n",
        "### Using LangGraph - Agent Chain with multiple tools"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's look at the final visualization.\n",
        "\n",
        "![image](https://i.imgur.com/NWO7usO.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3_fLDElOVoop"
      },
      "source": [
        "### Dependencies\n",
        ". . ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KaVwN269EttM",
        "outputId": "9d599609-f2cf-4702-ed03-6598fad3cf85"
      },
      "outputs": [],
      "source": [
        "!pip install -qU langchain langchain_openai langgraph arxiv duckduckgo-search tavily-python"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wujPjGJuoPwg"
      },
      "source": [
        "### Environment Variables\n",
        "OpenAI API, Tavily and LangSmith environment variables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jdh8CoVWHRvs",
        "outputId": "9b702489-3274-4d56-f0c2-7e7c788d8f53"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import getpass\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key:\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "os.environ[\"TAVILY_API_KEY\"] = getpass.getpass()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nv0glIDyHmRt",
        "outputId": "9a2f5160-bba4-4e17-867e-ef889d8ccbad"
      },
      "outputs": [],
      "source": [
        "from uuid import uuid4\n",
        "\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = f\"AIE2 - LangGraph - {uuid4().hex[0:8]}\"\n",
        "os.environ[\"LANGCHAIN_API_KEY\"] = getpass.getpass(\"LangSmith API Key: \")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sBRyQmEAVzua"
      },
      "source": [
        "### Agent Tool Belt\n",
        "\n",
        "Adding tools that equip our agent with a toolbelt to help answer questions and add external knowledge."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "lAxaSvlfIeOg"
      },
      "outputs": [],
      "source": [
        "#from langchain_community.tools.ddg_search import DuckDuckGoSearchRun\n",
        "from langchain_community.tools.arxiv.tool import ArxivQueryRun\n",
        "from langchain_community.tools.tavily_search import TavilySearchResults\n",
        "\n",
        "tool_belt = [\n",
        "#    DuckDuckGoSearchRun(),\n",
        "    TavilySearchResults(), # TavilyRun() replaced duckduckgo-search because of rate limiting\n",
        "    ArxivQueryRun() # Maybe use a different tool here for sentiment and all\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1FdOjEslXdRR"
      },
      "source": [
        "### Actioning with Tools\n",
        "The Agent will use a ToolExecutor "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "cFr1m80-JZsD"
      },
      "outputs": [],
      "source": [
        "from langgraph.prebuilt import ToolExecutor\n",
        "\n",
        "tool_executor = ToolExecutor(tool_belt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VI-C669ZYVI5"
      },
      "source": [
        "### Model\n",
        "\n",
        "Using OpenAI LLM, leveraging the OpenAI function calling API."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "QkNS8rNZJs4z"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "model = ChatOpenAI(temperature=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ugkj3GzuZpQv"
      },
      "source": [
        "OK, let's \"put on the tool belt\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "4OdMqFafZ_0V"
      },
      "outputs": [],
      "source": [
        "from langchain_core.utils.function_calling import convert_to_openai_function\n",
        "\n",
        "functions = [convert_to_openai_function(t) for t in tool_belt]\n",
        "model = model.bind_functions(functions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_296Ub96Z_H8"
      },
      "source": [
        "### The Agent State Machine\n",
        "LangGraph StatefulGraph - AgentState object.\n",
        "\n",
        "1. We initialize our state object:\n",
        "  - `{\"messages\" : []}`\n",
        "2. Our user submits a query to our application.\n",
        "  - New State: `HumanMessage(#1)`\n",
        "  - `{\"messages\" : [HumanMessage(#1)}`\n",
        "3. We pass our state object to an Agent node which is able to read the current state. It will use the last `HumanMessage` as input. It gets some kind of output which it will add to the state.\n",
        "  - New State: `AgentMessage(#1, additional_kwargs {\"function_call\" : \"WebSearchTool\"})`\n",
        "  - `{\"messages\" : [HumanMessage(#1), AgentMessage(#1, ...)]}`\n",
        "4. We pass our state object to a \"conditional node\" (more on this later) which reads the last state to determine if we need to use a tool - which it can determine properly because of our provided object!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "mxL9b_NZKUdL"
      },
      "outputs": [],
      "source": [
        "from typing import TypedDict, Annotated\n",
        "from langgraph.graph.message import add_messages\n",
        "import operator\n",
        "from langchain_core.messages import BaseMessage\n",
        "\n",
        "class AgentState(TypedDict):\n",
        "  messages: Annotated[list, add_messages]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vWsMhfO9grLu"
      },
      "source": [
        "### Build the Graph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "91flJWtZLUrl"
      },
      "outputs": [],
      "source": [
        "from langgraph.prebuilt import ToolInvocation\n",
        "import json\n",
        "from langchain_core.messages import FunctionMessage\n",
        "\n",
        "def call_model(state):\n",
        "  messages = state[\"messages\"]\n",
        "  response = model.invoke(messages)\n",
        "  return {\"messages\" : [response]}\n",
        "\n",
        "def call_tool(state):\n",
        "  last_message = state[\"messages\"][-1]\n",
        "\n",
        "  action = ToolInvocation(\n",
        "      tool=last_message.additional_kwargs[\"function_call\"][\"name\"],\n",
        "      tool_input=json.loads(\n",
        "          last_message.additional_kwargs[\"function_call\"][\"arguments\"]\n",
        "      )\n",
        "  )\n",
        "\n",
        "  response = tool_executor.invoke(action)\n",
        "\n",
        "  function_message = FunctionMessage(content=str(response), name=action.tool)\n",
        "\n",
        "  return {\"messages\" : [function_message]}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "# AI MAKERSPACE PREPR \n",
        "# Date: 2024-5-16\n",
        "\n",
        "# Basic Imports & Setup\n",
        "# import os\n",
        "# from openai import AsyncOpenAI\n",
        "\n",
        "from langchain.agents import Tool\n",
        "\n",
        "# Using Chainlit for our UI\n",
        "# import chainlit as cl\n",
        "# from chainlit.prompt import Prompt, PromptMessage\n",
        "# from chainlit.playground.providers import ChatOpenAI\n",
        "\n",
        "# Getting the API key from the .env file\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()\n",
        "\n",
        "# RAG pipeline imports and setup code\n",
        "# Get the DeveloperWeek PDF file (future implementation: direct download from URL)\n",
        "from langchain.document_loaders import PyMuPDFLoader\n",
        "\n",
        "# Adjust the URL to the direct download format\n",
        "file_id = \"1JeA-w4kvbI3GHk9Dh_j19_Q0JUDE7hse\"\n",
        "direct_url = f\"https://drive.google.com/uc?export=download&id={file_id}\"\n",
        "\n",
        "# Now load the document using the direct URL\n",
        "docs = PyMuPDFLoader(direct_url).load()\n",
        "\n",
        "import tiktoken\n",
        "def tiktoken_len(text):\n",
        "    tokens = tiktoken.encoding_for_model(\"gpt-3.5-turbo\").encode(\n",
        "        text,\n",
        "    )\n",
        "    return len(tokens)\n",
        "\n",
        "# Split the document into chunks\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size = 500,           # 500 tokens per chunk, experiment with this value\n",
        "    chunk_overlap = 50,        # 50 tokens overlap between chunks, experiment with this value\n",
        "    length_function = tiktoken_len,\n",
        ")\n",
        "\n",
        "split_chunks = text_splitter.split_documents(docs)\n",
        "\n",
        "# Load the embeddings model\n",
        "from langchain_openai.embeddings import OpenAIEmbeddings\n",
        "\n",
        "embedding_model = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
        "\n",
        "# Load the vector store and retriever from Qdrant\n",
        "from langchain_community.vectorstores import Qdrant\n",
        "\n",
        "qdrant_vectorstore = Qdrant.from_documents(\n",
        "    split_chunks,\n",
        "    embedding_model,\n",
        "    location=\":memory:\",\n",
        "    collection_name=\"Prepr\",\n",
        ")\n",
        "\n",
        "qdrant_retriever = qdrant_vectorstore.as_retriever()\n",
        "\n",
        "from langchain_openai import ChatOpenAI\n",
        "openai_chat_model = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
        "\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "RAG_PROMPT = \"\"\"\n",
        "SYSTEM:\n",
        "You are a professional personal assistant.\n",
        "You are a helpful personal assistant who provides information about conferences.\n",
        "You like to provide helpful responses to busy professionals who ask questions about conferences.\n",
        "\n",
        "You can have a long conversation with the user about conferences.\n",
        "When to talk with the user about conferences, it can be a \"transactional conversation\" with a prompt-response format with one prompt from the user followed by a response by you.\n",
        "\n",
        "Here is an example of a transactional conversation:\n",
        "User: When is the conference?\n",
        "You: The conference is on June 1st, 2024. What else would you like to know?\n",
        "\n",
        "It can also be a chain of questions and answers where you and the user continues the chain until they say \"Got it\".\n",
        "Here is an example of a transactional conversation:\n",
        "User: What sessions should I attend?\n",
        "You: You should attend the keynote session by Bono. Would you like to know more?\n",
        "User: Yes\n",
        "You: The keynote session by Bono is on June 1st, 2024. What else would you like?\n",
        "\n",
        "If asked a question about a sessions, you can provide detailed information about the session.\n",
        "If there are multiple sessions, you can provide information about each session.\n",
        "\n",
        "The format of session related replies is:\n",
        "Title:\n",
        "Description:\n",
        "Speaker:\n",
        "Background:\n",
        "Date:\n",
        "Topics to Be Covered:\n",
        "Questions to Ask:\n",
        "\n",
        "CONTEXT:\n",
        "{context}\n",
        "\n",
        "QUERY:\n",
        "{question}\n",
        "Most questions are about the date, location, and purpose of the conference.\n",
        "You may be asked for fine details about the conference regarding the speakers, sponsors, and attendees.\n",
        "You are capable of looking up information and providing detailed responses.\n",
        "When asked a question about a conference, you should provide a detailed response.\n",
        "After completing your response, you should ask the user if they would like to know more about the conference by asking \"Hope that helps\".\n",
        "If the user says \"yes\", you should provide more information about the conference. If the user says \"no\", you should say \"Goodbye! or ask if they would like to provide feedback.\n",
        "If you are asked a question about Cher, you should respond with \"Rock on With Your Bad Self!\".\n",
        "If you can not answer the question, you should say \"I am sorry, I do not have that information, but I am always here to help you with any other questions you may have.\".\n",
        "\"\"\"\n",
        "rag_prompt = ChatPromptTemplate.from_template(RAG_PROMPT)\n",
        "\n",
        "from operator import itemgetter\n",
        "from langchain.schema.output_parser import StrOutputParser\n",
        "from langchain.schema.runnable import RunnablePassthrough\n",
        "\n",
        "retrieval_augmented_qa_chain = (\n",
        "        {\"context\": itemgetter(\"question\") | qdrant_retriever, \"question\": itemgetter(\"question\")}\n",
        "        | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "        | {\"response\": rag_prompt | openai_chat_model, \"context\": itemgetter(\"context\")}\n",
        ")\n",
        "\n",
        "retrieval_augmented_qa_chain.invoke({\"question\": 'whens the event coming?'})\n",
        "\n",
        "rag_tool = Tool(\n",
        "    name=\"RAGTool\",\n",
        "    func=lambda inputs: retrieval_augmented_qa_chain.invoke(inputs),\n",
        "    description=\"Use this tool to answer questions using the RAG approach.\"\n",
        ")\n",
        "\n",
        "from langgraph.graph import StateGraph, END\n",
        "\n",
        "# Define a state schema\n",
        "state_schema = {\n",
        "    \"start\": \"agent\",\n",
        "    \"states\": {\n",
        "        \"agent\": {\n",
        "            \"next\": \"action\",\n",
        "            \"type\": \"function\",\n",
        "            \"function\": lambda context: {\"input\": context[\"input\"]}\n",
        "        },\n",
        "        \"action\": {\n",
        "            \"next\": END,\n",
        "            \"type\": \"tool\",\n",
        "            \"tool\": rag_tool\n",
        "        }\n",
        "    }\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2bwR7MgWj3Wg"
      },
      "source": [
        "Now we have two total nodes. We have:\n",
        "- `call_model` is a node that will...well...call the model\n",
        "- `call_tool` is a node which will call a tool"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "_vF4_lgtmQNo"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/mcshrub/miniconda3/envs/mikec-aim/lib/python3.11/site-packages/langgraph/graph/state.py:45: UserWarning: Invalid state_schema: {'start': 'agent', 'states': {'agent': {'next': 'action', 'type': 'function', 'function': <function <lambda> at 0x12aa90900>}, 'action': {'next': '__end__', 'type': 'tool', 'tool': Tool(name='RAGTool', description='Use this tool to answer questions using the RAG approach.', func=<function <lambda> at 0x12aa90540>)}}}. Expected a type or Annotated[type, reducer]. Please provide a valid schema to ensure correct updates.\n",
            " See: https://langchain-ai.github.io/langgraph/reference/graphs/#stategraph\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from langgraph.graph import StateGraph, END\n",
        "\n",
        "workflow = StateGraph(state_schema=state_schema)\n",
        "\n",
        "workflow.add_node(\"agent\", call_model)\n",
        "workflow.add_node(\"action\", call_tool)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uaXHpPeSnOWC"
      },
      "source": [
        "Next, we'll add our entrypoint. All our entrypoint does is indicate which node is called first."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "YGCbaYqRnmiw"
      },
      "outputs": [],
      "source": [
        "workflow.set_entry_point(\"agent\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Q_pQgHmoW0M"
      },
      "source": [
        "Now we want to build a \"conditional edge\" which will use the output state of a node to determine which path to follow. We can help conceptualize this by thinking of our conditional edge as a conditional in a flowchart!\n",
        "\n",
        "Then we create an edge where the origin node is our agent node and our destination node is *either* the action node or the END (finish the graph)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "1BZgb81VQf9o"
      },
      "outputs": [],
      "source": [
        "def should_continue(state):\n",
        "  last_message = state[\"messages\"][-1]\n",
        "\n",
        "  if \"function_call\" not in last_message.additional_kwargs:\n",
        "    return \"end\"\n",
        "\n",
        "  return \"continue\"\n",
        "\n",
        "workflow.add_conditional_edges(\n",
        "    \"agent\",\n",
        "    should_continue,\n",
        "    {\n",
        "        \"continue\" : \"action\",\n",
        "        \"end\" : END\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yKCjWJCkrJb9"
      },
      "source": [
        "Finally, we can add our last edge which will connect our action node to our agent node. This is because we *always* want our action node (which is used to call our tools) to return its output to our agent!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "UvcgbHf1rIXZ"
      },
      "outputs": [],
      "source": [
        "workflow.add_edge(\"action\", \"agent\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KYqDpErlsCsu"
      },
      "source": [
        "All that's left to do now is to compile our workflow - and we're off!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "zt9-KS8DpzNx"
      },
      "outputs": [],
      "source": [
        "app = workflow.compile()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GSCds6zTL5VJ"
      },
      "source": [
        "#### Helper Function to print messages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "xRPF0X5iL8Bh"
      },
      "outputs": [],
      "source": [
        "def print_messages(messages):\n",
        "  next_is_tool = False\n",
        "  initial_query = True\n",
        "  for message in messages[\"messages\"]:\n",
        "    if \"function_call\" in message.additional_kwargs:\n",
        "      print()\n",
        "      print(f'Tool Call - Name: {message.additional_kwargs[\"function_call\"][\"name\"]} + Query: {message.additional_kwargs[\"function_call\"][\"arguments\"]}')\n",
        "      next_is_tool = True\n",
        "      continue\n",
        "    if next_is_tool:\n",
        "      print(f\"Tool Response: {message.content}\")\n",
        "      next_is_tool = False\n",
        "      continue\n",
        "    if initial_query:\n",
        "      print(f\"Initial Query: {message.content}\")\n",
        "      print()\n",
        "      initial_query = False\n",
        "      continue\n",
        "    print()\n",
        "    print(f\"Agent Response: {message.content}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VEYcTShCsPaa"
      },
      "source": [
        "## Using Our Graph\n",
        "\n",
        "Now that we've created and compiled our graph - we can call it *just as we'd call any other* `Runnable`!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "Qn4n37PQRPII",
        "outputId": "1f3fb168-9f1a-4853-dbfe-dda63971dd62"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initial Query: I found some information about the DeveloperWeek 2024 conference. Here are some details:\n",
            "\n",
            "1. **Conference Schedule**: DeveloperWeek 2024 will take place from February 21-29, 2024, in the San Francisco Bay Area and virtually. It is described as the world's largest developer and engineering technology conference and expo. You can register for the event on their [official website](https://www.developerweek.com/conference/).\n",
            "\n",
            "2. **Event Details**: The conference will include various activities such as workshops, hackathons, expo, conference sessions, awards ceremony, and networking opportunities. The schedule is packed with different events each day from February 21-29, 2024. You can find the full schedule on their [events page](https://www.developerweek.com/events/).\n",
            "\n",
            "3. **Virtual Participation**: For those attending virtually, the conference will have online workshops, virtual expo, and conference sessions from February 27-29, 2024. You can find more details on the [DeveloperWeek 2024 schedule page](https://developerweek2024.sched.com/).\n",
            "\n",
            "4. **Keynotes and Speakers**: The conference will feature keynote speakers including founders, CEOs, senior directors, and other industry leaders. You can learn more about the keynotes and speakers on the [DeveloperWeek website](https://www.developerweek.com/).\n",
            "\n",
            "5. **About DeveloperWeek**: DeveloperWeek is an annual gathering of developers, engineers, software architects, dev teams, managers, and executives from over 70 countries. The event focuses on the latest developer technologies, languages, platforms, and tools. You can contact them for more information about hosting workshops or seminars as part of DeveloperWeek.\n",
            "\n",
            "If you need more specific information or have any other questions, feel free to ask!\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.messages import HumanMessage\n",
        "\n",
        "inputs = {\"messages\" : [HumanMessage(content=\"When is DeveloperWeek conference scheduled?\")]}\n",
        "#inputs = {\"messages\" : [HumanMessage(content=\"What is RAG?\")]}\n",
        "messages = app.invoke(inputs)\n",
        "\n",
        "print_messages(messages)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DBHnUtLSscRr"
      },
      "source": [
        "1. Our state object was populated with our request\n",
        "2. The state object was passed into our entry point (agent node) and the agent node added an `AIMessage` to the state object and passed it along the conditional edge\n",
        "3. The conditional edge received the state object, found the \"function_call\" `additional_kwarg`, and sent the state object to the action node\n",
        "4. The action node added the response from the OpenAI function calling endpoint to the state object and passed it along the edge to the agent node\n",
        "5. The agent node added a response to the state object and passed it along the conditional edge\n",
        "6. The conditional edge received the state object, could not find the \"function_call\" `additional_kwarg` and passed the state object to END where we see it output in the cell above!\n",
        "\n",
        "Now let's look at an example that shows a multiple tool usage - all with the same flow!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "afv2BuEsV5JG",
        "outputId": "4d732ee1-5645-4c01-b236-7eec7f806753"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initial Query: Here are some search results related to Al Gore and the invention of the Internet:\n",
            "\n",
            "1. [The Fact Checker - A cautionary tale for politicians: Al Gore and the invention of the Internet](https://www.washingtonpost.com/news/fact-checker/wp/2013/11/04/a-cautionary-tale-for-politicians-al-gore-and-the-invention-of-the-internet/): This article discusses the claim made by Al Gore about inventing the Internet.\n",
            "\n",
            "2. [Mental Floss - History of the U.S.: Al Gore Really Did \"Take the Initiative in Creating the Internet\"](https://www.mentalfloss.com/article/25986/history-us-al-gore-really-did-take-initiative-creating-internet): An article exploring Al Gore's role in the creation of the Internet.\n",
            "\n",
            "3. [Origins - Gore Did Help Invent Internet](https://origins.osu.edu/history-news/gore-did-help-invent-internet?language_content_entity=en): This article provides a historical perspective on Al Gore's involvement in the development of the Internet.\n",
            "\n",
            "4. [Wikipedia - Al Gore and Information Technology](https://en.wikipedia.org/wiki/Al_Gore_and_information_technology): Information about Al Gore's contributions to information technology, including his work on the High Performance Computing and Communication Act of 1991.\n",
            "\n",
            "5. [Snopes - Internet of Lies](https://www.snopes.com/fact-check/internet-of-lies/): Debunks the claim that Al Gore claimed he \"invented\" the Internet and provides context on the origin of the misleading information.\n",
            "\n",
            "These sources provide a range of perspectives on Al Gore's role in the development of the Internet.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "inputs = {\"messages\" : [HumanMessage(content=\"When did Al Gore created the Internet.  What year did Trump go to jail?\")]}\n",
        "\n",
        "messages = app.invoke(inputs)\n",
        "\n",
        "print_messages(messages)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pV3XeFOT1Sar"
      },
      "source": [
        "### EXTRA \n",
        "### Pre-processing for LangSmith"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wruQCuzewUuO"
      },
      "source": [
        "To do a little bit more preprocessing, let's wrap our LangGraph agent in a simple chain."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "oeXdQgbxwhTv"
      },
      "outputs": [],
      "source": [
        "def convert_inputs(input_object):\n",
        "  return {\"messages\" : [HumanMessage(content=input_object[\"question\"])]}\n",
        "\n",
        "def parse_output(input_state):\n",
        "  return input_state[\"messages\"][-1].content\n",
        "\n",
        "agent_chain = convert_inputs | app | parse_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "orYxBZXSxJjZ",
        "outputId": "6eda06b2-0110-44c0-8106-b1280376a2c3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'RAG (Retrieval Augmented Generation) for LLM (Large Language Models) applications allows us to give foundational models local context without expensive fine-tuning. It can be done on normal everyday machines like laptops. If you are interested in getting started with RAG for LLM-powered applications, you can refer to tutorials and guides available online:\\n\\n1. [LLM RAG Tutorial](https://colab.research.google.com/github/SamHollings/llm_tutorial/blob/main/llm_tutorial_rag.ipynb): This tutorial provides a simple introduction to getting started with an LLM to create a RAG app.\\n\\n2. [Your Guide to Starting With RAG for LLM-Powered Applications](https://medium.com/@caldhubaib/your-guide-to-starting-with-rag-for-llm-powered-applications-ee5ce31cab71): This guide offers insights and tips on building enterprise-grade LLMs and getting started with RAG for LLMs.\\n\\n3. [Basic Conversational AI with RAG Solutions](https://github.com/zahaby/intro-llm-rag): This guide is for technical teams interested in developing a basic conversational AI with RAG solutions. It provides an introduction to the technical aspects and code implementation.\\n\\n4. [Building RAG Application Using a LLM Running on Local Computer](https://medium.com/rahasak/build-rag-application-using-a-llm-running-on-local-computer-with-ollama-and-langchain-e6513853fda0): This article discusses building a RAG application using the Llama2 LLM running with Ollama to provide answers based on content in the Open5GS documentation.\\n\\n5. [Comprehensive Guide for Building RAG-Based LLM Applications](https://www.anyscale.com/blog/a-comprehensive-guide-for-building-rag-based-llm-applications-part-1): This guide covers various aspects of building RAG-based LLM applications, including monitoring and debugging Ray applications and clusters.\\n\\nThese resources can help you understand and implement RAG for LLM applications effectively.'"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "agent_chain.invoke({\"question\" : \"What is RAG for LLM Applications\"})"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
